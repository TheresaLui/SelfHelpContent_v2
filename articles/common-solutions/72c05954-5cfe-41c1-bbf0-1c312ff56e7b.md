<properties
  pagetitle="Mapping Data Flow - Data Flow Activity Common Solutions"
  description=""
  service=""
  resource=""
  ms.author="rakatuko"
  selfhelptype="Generic"
  supporttopicids="32633533"
  productpesids="15613"
  cloudenvironments="fairfax,public,usnat,ussec"
  disableclouds="blackforest,mooncake"
  articleid="72c05954-5cfe-41c1-bbf0-1c312ff56e7b"
  ownershipid="AzureData_DataFactory" />
# Mapping Data Flow - Data Flow Activity Common Solutions

## Overview
This article explores common scenarios helpful in troubleshooting around Data Flow Pipeline/Activity run issues. 

Broadly, Data Flow Activity runs can be categorized as Debug Runs and Trigger Runs. 

- [Debug Activity Runs](https://docs.microsoft.com/azure/data-factory/iterative-development-debugging#debugging-a-pipeline-with-a-data-flow-activity)

- [Trigger Activity Runs](https://docs.microsoft.com/azure/data-factory/concepts-data-flow-monitoring)

### Common Issues
- ***Run Failed with "Internal Server Error"*** 

    There could be many potential scenarios for this issue:
    1. ***Choosing the right compute size/type***

        ***Cause***: Successful execution of data flow depends on many factors including compute size/type, number of source/sinks to process, partition specification, transformations involved, size of datasets, data skewness, etc.

        ***Resolution***: [Integration Runtime](https://docs.microsoft.com/azure/data-factory/concepts-data-flow-performance#ir)   

    2. ***Using Debug session with parallel activities***

        ***Cause***: Triggering a run using data flow debug session with constructs like ForEach in the pipeline may submit multiple parallel runs to the same cluster. This may lead to issues with cluster failure while running these jobs due to resource issues like Out Of Memory.

        ***Resolution***: Use "Trigger Now" or "Debug -> Use Activity Runtime" option to submit run with appropriate IR configuration defined in the pieline activity and after publishing the changes.

    3. ***Transient Issues***

        ***Cause***: Transient issues with micro-services involved in the execution may cause the run to fail.

        ***Resolution***: Configuring retries in the pipeline activity could help resolve the issues due to  transient issues. Refer [Activity Policy](https://docs.microsoft.com/azure/data-factory/concepts-pipelines-activities#activity-policy)


- ***Cluster Acquisition taking too long***

    Data flow uses Spark behind the scene and spin clusters, if you want to reduce the clsuter startup time you can use [Time-To-Live](https://docs.microsoft.com/azure/data-factory/concepts-data-flow-performance#time-to-live) feature.


- ***There are substantial concurrent Mapping Data flow executions which is causing failures due to throttling under Integration Runtime 'IRName', ActivityId: 'activityId'***

    ***Cause***: Number of concurrent data flow runs allowed per Integration Runtime is limited to 50 ([Dataflow Run Limits](https://docs.microsoft.com/azure/azure-resource-manager/management/azure-subscription-service-limits#version-2)).

    ***Resolution***: Submitting too many runs within a short span could lead to throttling of customer runs. Customers should review their pipeline design minimizing substantial concurrent runs, distribute runs in different time slots, etc. to avoid running into throttling issues.

    [An example Scenario](https://kromerbigdata.com/2019/07/05/adf-mapping-data-flows-iterate-multiple-files-with-source-transformation/)


- ***Pipeline is taking longer to execute compared to previous runs***

    ***Cause***: This could be result of recent changes to the dataflow definition adding transforms like join, window, sort, etc. which are more resource intensive or changes in the dataset itself.

    ***Resolution***: Review the changes to the data flow in accordance to [Performance tuning guide](https://docs.microsoft.com/azure/data-factory/concepts-data-flow-performance#optimizing-transformations). Also review if dataset size changed significantly in comparison to the earlier runs.


- ***Bad Request User Error due to incorrect Integration Runtime Configurations***

    ***Cause***: Activity run is submitted using the SDK, powershell, etc. with customer Integration Runtime configurations.

    ***Resolution***:
    Data Flow works on a predefined compute configurations and fails the request for any un-supported configuration. Refer: [Integration Runtime](https://docs.microsoft.com/azure/data-factory/concepts-data-flow-performance#ir)

## **Recommended Documents**

- [Azure Data Factory FAQ](https://docs.microsoft.com/azure/data-factory/frequently-asked-questions)
- [Feature Request](https://feedback.azure.com/forums/270578-azure-data-factory)