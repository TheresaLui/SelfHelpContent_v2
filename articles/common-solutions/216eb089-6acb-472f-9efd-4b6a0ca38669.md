<properties
  pagetitle="Problems with Model Explanations in Experiments&#xD;"
  service="microsoft.machinelearningservices"
  resource="workspaces"
  ms.author="mithigpe"
  selfhelptype="Generic"
  supporttopicids="32745200"
  resourcetags=""
  productpesids="16644"
  cloudenvironments="public,fairfax,mooncake,blackforest,ussec,usnat"
  articleid="216eb089-6acb-472f-9efd-4b6a0ca38669"
  ownershipid="AzureML_AzureMachineLearning" />
# Problems with Model Explanations in Experiments
Most users are able to resolve their model explanation issue using the steps below.

## **Recommended Steps**
### Uploading a model explanation dashboard from the SDK
Create a training script in a Jupyter notebook to submit a [remote explanation](https://docs.microsoft.com/azure/machine-learning/how-to-machine-learning-interpretability-aml#generate-feature-importance-values-via-remote-runs) by setting up AzureML Compute target as your compute target and submitting your remote run. 

The steps shown in these [sample notebooks](https://github.com/Azure/MachineLearningNotebooks/tree/master/how-to-use-azureml/explain-model/azure-integration/remote-explanation) may be useful.

### Accessing the model explanation dashboard from the Experiments UI

After you've uploaded your model explanations to Azure Machine Learning Run History, you can view the dashboard in Azure Machine Learning studio UI by using the following steps:

1. Select **Experiments** in the left pane to see a list of experiments that you've run on Azure Machine Learning
2. Select a particular experiment to view all the runs in that experiment
3. Select a run, and then the click the Explanations tab to view the explanation visualization dashboard

### Known Issues

We are working on solutions to support the following:

- Getting local feature importance values for sparse datasets.
- Getting the dataset index/ID of a selected instance on Dataset explorer and Individual feature importance view. If you pass in your raw dataset with associated feature values you will be able to view your Row Identifier feature and map local importances back to your dataset. In the instance of AutoML generated explanations, the dataset is randomly downsampled so even the dataset index will not be accurately mapped to the right dataset index in your original dataset.

## **Recommended Documents**

* For more information about model interpretability in machine learning, see [the concept page](https://docs.microsoft.com/azure/machine-learning/how-to-machine-learning-interpretability)
* For how-to on model explanations with AutoML,see [the how-to page](https://docs.microsoft.com/azure/machine-learning/how-to-machine-learning-interpretability-automl)
