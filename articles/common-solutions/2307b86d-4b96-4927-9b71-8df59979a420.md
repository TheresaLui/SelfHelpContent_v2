<properties
  pagetitle="Unexpected increase in Resource Consumption causing Slow performance"
  service="microsoft.azuredata"
  resource="postgresinstances"
  ms.author="pookam"
  selfhelptype="Generic"
  supporttopicids="32747934"
  productpesids="17124"
  cloudEnvironments="public, fairfax, usnat, ussec"
  articleid="2307b86d-4b96-4927-9b71-8df59979a420"
  ownershipid="AzureData_AzureDatabaseforPostgreSQL" />
# Unexpected increase in Resource Consumption causing Slow performance

Increase in resource consumption can be a result of an explicit user action or changes in the workload. Sometimes increased CPU utilization can also be caused by an issue with PostgreSQL internal processes, such as autovacuum not running as expected.

## **Recommended Steps**

* Check if you have increased resource consumption View logs using [Searching logs with Kibana](https://docs.microsoft.com/azure/azure-arc/data/monitor-grafana-kibana)
* Ensure there are no changes to the worker nodes that might have triggered the increase / Scale out to commensurate to the increase in the workload
* Check if there was any schema changes, for example:

    * Was an index was dropped?
    * If you added columns, do you need a new index?

* Check if a significant proportion of the data was changed. If so, did you update the indexes?
* Ensure that the table statistics are up to date
* Review when your tables were last vacuumed and tune threshold parameters (see documentation below)
* [View Logs using Grafana & Kibana](https://docs.microsoft.com/azure/azure-arc/data/monitor-grafana-kibana). Go to the Metrics Dashboard link provided using the above steps and log in with the same credentials you use for azdata login
* Here, there are 2 things you need to determine. If this is a Hyperscale system, is one node impacted and the rest fine, or are all affected?
		
	* If only one worker is impacted, there may be a distribution key seeing more traffic than the others
	* If all nodes are showing similar behavior, then it is probably systemic load and should be approached using below information

* What kind of load are you seeing?

	* If you are seeing high CPU usage, that is typically a symptom of either heavy insert activity, maintenance processes such as VACUUM, or heavy analytics queries
	* If you are seeing high write activity, look for large inserts, updates, or VACUUM
	* If you are seeing high read activity look at VACUUM and long-running queries
   
* CPU bound:
		
	* Look at computationally expensive queries, long rollups, and VACUUM activity

* I/O bound:
		
	* Look for large data ingests, VACUUM activity, and/or full table scans on common queries

* This query will look at pg_stat_statements and give you a good picture of major load contributors. If any stand out, tune those queries as needed:
  
```
select rolname,total_time,calls,mean_time,regexp_replace(pss.query,'[ \t\n]+', ' ', 'g') AS query_text
FROM pg_stat_statements pss
JOIN pg_roles r ON r.oid=userid
WHERE calls > 100
AND rolname NOT LIKE '%backup'
ORDER BY total_time DESC
limit 20;
 ```
 
*  [Check for unused indices](http://docs.citusdata.com/en/v9.2/admin_guide/diagnostic_queries.html#identifying-unused-indices)

## **Recommended Documents**

- [Useful Diagnostic Queries](http://docs.citusdata.com/en/v9.4/admin_guide/diagnostic_queries.html) 
- [Tuning your PostgreSQL Server](https://wiki.postgresql.org/wiki/Tuning_Your_PostgreSQL_Server) 
- [Getting logs for Azure Arc enabled data services](https://github.com/TheJY/azure-docs-pr/blob/release-ignite-arc-data/articles/azure-arc/data/troubleshooting-get-logs.md)
- [Searching logs with Kibana](https://github.com/TheJY/azure-docs-pr/blob/release-ignite-arc-data/articles/azure-arc/data/monitor-grafana-kibana.md)
- [Monitoring with Grafana](https://github.com/TheJY/azure-docs-pr/blob/release-ignite-arc-data/articles/azure-arc/data/monitor-grafana-kibana.md)
- [Troubleshooting Postgres Hyperscale server groups](https://docs.microsoft.com/azure/azure-arc/data/troubleshoot-postgresql-hyperscale-server-group)
