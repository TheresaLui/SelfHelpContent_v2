<properties
 pageTitle="ClassNotFoundException or NoClassDefFound"
 description="ClassNotFoundException or NoClassDefFound"
 service="Microsoft.Databricks"
 resource="Microsoft.Databricks/workspaces"
 authors="AzureData_AzureDatabricks"
 ms.author="AzureData_AzureDatabricks"
 displayOrder=""
 selfHelpType="TSG_Content"
 supportTopicIds=""
 resourceTags=""
 productPesIds=""
 cloudEnvironments="public, fairfax, usnat, ussec"
 articleId="ca099390-9d9e-4ba5-ac8b-b5dbba1bb14c"
 ownershipId="AzureData_AzureDatabricks"
/>

# ClassNotFoundException or NoClassDefFound

### Symptom

This happens when the library is not present on the classpath. This is typical in situations when you are providing a jar through the User Interface and the jar supplied for a library  is outside of Databricks that will need to be accessible on every node in the cluster. The actual error in the driver logs will be similar to: 

```
Exception in thread 'main' java.lang.NoClassDefFoundError: org/apache/hadoop/conf/Configuration
...
...
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.conf.Configuration
at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
...
... 
```

### Troubleshooting


This is typical in situations when you are providing a jar through the User Interface and the jar supplied for a library outside of Databricks that will need to be accessible on every node in the cluster. Verify with customer which jar they are using that is required to determine if we are providing this already in our environment or if it has been created using Maven or from another method. 

### Solution

We usually use an init script to ensure that the jar is included in the root class using the following commands from a notebook attached to the same cluster: 

- Create a directory within a notebook attached to the cluster :

  `dbutils.fs.mkdirs(""dbfs:/<directory_name>//"") `
 
- Create an init script that will copy the jar from the DBFS to the the appropriate locations for it to be recognized by the driver/workers: 

  ```
  dbutils.fs.put(""dbfs:/<directory_name>/install_jar.sh"",""""""
  #!/bin/bash
  sleep 20
  cp /dbfs/FileStore/jars/<added_.jar_name>.jar /databricks/jars/ """""")
  ```

### Next Steps

If issue persists, involve SME on Ava and file a Databricks SF ticket if needed.
