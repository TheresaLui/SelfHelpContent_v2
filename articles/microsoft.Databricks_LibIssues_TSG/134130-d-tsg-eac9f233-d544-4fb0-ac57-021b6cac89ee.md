<properties
 pageTitle=" An error occurred while initializing the REPL"
 description=" An error occurred while initializing the REPL"
 service="Microsoft.Databricks"
 resource="Microsoft.Databricks/workspaces"
 authors="AzureData_AzureDatabricks"
 ms.author="AzureData_AzureDatabricks"
 displayOrder=""
 selfHelpType="TSG_Content"
 supportTopicIds=""
 resourceTags=""
 productPesIds=""
 cloudEnvironments="public, fairfax, usnat, ussec"
 articleId="eac9f233-d544-4fb0-ac57-021b6cac89ee"
 ownershipId="AzureData_AzureDatabricks"
/>

#  An error occurred while initializing the REPL

### Symptom

This error `java.lang.Exception: An error occurred while initializing the REPL` usually occurs when there are conflicting Scala libraries or JARs attached to the cluster, such as Scala 2.11 libraries attached to Scala 2.10 cluster. 


### Troubleshooting


Please check whether there are conflicting Scala libraries or JARs attached to the cluster, such as Scala 2.11 libraries attached to Scala 2.10 cluster (or vice-versa).

    - You can confirm by removing any new jar and running a test in the notebook and then adding one jar at a time and verifying if the same issues occur. 
    - After this, we can use `spark.driver.extraJavaOptions verbose:class` and compare the classes being loaded in the Spark UI > Environment > scroll to the bottom and search the classes to find the problematic library or use the jar command `jar tf jar-file`.

### Solution

By default, maven does not include dependency jars when it builds a target. When running a Spark job, if the Spark worker machines don't contain the dependency jars, there will be an error that a class cannot be found.

The easiest way to work around this is to create a shaded or uber jar to package the dependencies in the jar as well.

It is possible to opt out certain dependencies from being included in the uber jar by marking them as <scope>provided</scope>. Spark dependencies should be marked as provided since they are already on the Spark cluster. You may also exclude other jars that you have installed on your worker machines.

Here is [an example Maven uberjarexample.xml file](https://supportability.visualstudio.com/_git/AzureDataBricks?path=%2FGuidedTS_files%2Fuberjarexample.xml) that creates an uber jar with all the code in that project and includes the common-cli dependency, but not any of the Spark libraries.


### Next Steps

If issue persists, involve SME on Ava and file a Databricks SF ticket if needed.
