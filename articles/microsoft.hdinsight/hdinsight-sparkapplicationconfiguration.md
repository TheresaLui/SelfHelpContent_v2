<properties
    pageTitle="Spark application configuration"
    description="Spark application configuration"
    service="microsoft.hdinsight"
    resource="clusters"
    authors="bharathsreenivas"
    displayOrder="8"
    selfHelpType="resource"
    supportTopicIds="32511212"
    resourceTags=""
    productPesIds=""
    cloudEnvironments="public"
/>

# Spark application configuration

The amount of memory and number of cores that a Spark application can use can be configured through Ambari, spark-submit, LIVY or Jupyter notebooks.

## **Recommended documents**
[Spark Application Configuration through Ambari](https://hdinsight.github.io/spark/spark-application-configuration-through-ambari.html)<br>
[Spark Application Configuration through spark-submit](https://hdinsight.github.io/spark/spark-application-configuration-through-spark-submit.html)<br>
[Spark Application Configuration through LIVY](https://hdinsight.github.io/spark/spark-application-configuration-through-livy.html)<br>
[Spark Application Configuration through Jupyter Notebooks](https://hdinsight.github.io/spark/spark-application-configuration-through-jupyter.html)<br>
