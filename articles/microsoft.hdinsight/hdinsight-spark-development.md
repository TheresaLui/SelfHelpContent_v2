<properties
    pageTitle="Spark application configuration"
    description="Spark application configuration"
    service="microsoft.hdinsight"
    resource="clusters"
    authors="bharathsreenivas"
    ms.author="v-anukar"
    displayOrder=""
    selfHelpType="generic"
    supportTopicIds="32636495"
    resourceTags=""
    productPesIds="15078"
    cloudEnvironments="public, MoonCake"
    articleId="1cc6f5a8-476c-4638-9a5b-be97b0640439"
/>

# Spark Application Configuration

The amount of memory and number of cores that a Spark application can use can be configured through Ambari, spark-submit, LIVY, or Jupyter notebooks.

## **Recommended Documents**

* [Spark Application Configuration through Ambari](https://hdinsight.github.io/spark/spark-application-configuration-through-ambari.html)
* [Spark Application Configuration through spark-submit](https://hdinsight.github.io/spark/spark-application-configuration-through-spark-submit.html)
* [Spark Application Configuration through LIVY](https://hdinsight.github.io/spark/spark-application-configuration-through-livy.html)
* [Spark Application Configuration through Jupyter Notebooks](https://hdinsight.github.io/spark/spark-application-configuration-through-jupyter.html)
