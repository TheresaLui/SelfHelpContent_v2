<properties
    pageTitle="Spark application configuration"
    description="Spark application configuration"
    service="microsoft.hdinsight"
    resource="clusters"
    authors="bharathsreenivas"
    authorAlias="v-anukar"
    displayOrder=""
    selfHelpType="generic"
    supportTopicIds="32629133"
    resourceTags=""
    productPesIds="15078"
    cloudEnvironments="public, MoonCake"
/>

# Spark application configuration

The amount of memory and number of cores that a Spark application can use can be configured through Ambari, spark-submit, LIVY or Jupyter notebooks.

## **Recommended documents**
[Spark Application Configuration through Ambari](https://hdinsight.github.io/spark/spark-application-configuration-through-ambari.html)<br>
[Spark Application Configuration through spark-submit](https://hdinsight.github.io/spark/spark-application-configuration-through-spark-submit.html)<br>
[Spark Application Configuration through LIVY](https://hdinsight.github.io/spark/spark-application-configuration-through-livy.html)<br>
[Spark Application Configuration through Jupyter Notebooks](https://hdinsight.github.io/spark/spark-application-configuration-through-jupyter.html)<br>
